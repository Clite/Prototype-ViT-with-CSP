{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xirM-J4H6mf5",
        "5jH6rWlK8H4q",
        "Sns7ZkPnUKey",
        "uqfcR7pgJEsd",
        "BdBAyJ8SJNq3",
        "RcALOITcJoy9",
        "ofJ7uoL-JsAf",
        "A8FXaeBNMJY_"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Prototype ViT+CSP**\n",
        "\n",
        "*   **Package install & Library import**\n",
        "*   **Sub-design**\n",
        " *  Multi-Head Self-Attention layer\n",
        "*   **Hyperparameter & Dataset**\n",
        "*   **Vanilla ViT**\n",
        " *  Encoder\n",
        " *  ViT(Vision Transformer)\n",
        " *  Setting\n",
        " *  Train & Test\n",
        "*   **ViT+CSP**\n",
        " *  Encoder\n",
        " *  ViT+CSP\n",
        " *  Setting\n",
        " *  Train & Test\n",
        "*   **ViT+CSP version 2**\n",
        " *  MHA ver2\n",
        " *  Encoder\n",
        " *  ViT+CSP version2 \n",
        " *  Setting\n",
        " *  Train & Test\n",
        "*  **Spec comparison**\n",
        " *  Accuracy\n",
        " *  FLOPS & Parameters\n",
        " *  Inference time\n",
        "* **Model save & load (colab only)**"
      ],
      "metadata": {
        "id": "41LyCc1sU51t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Package install & Library import"
      ],
      "metadata": {
        "id": "xirM-J4H6mf5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEeLM9ZGH-C8"
      },
      "source": [
        "\"\"\" Install einops, torchsummary packages \"\"\"\n",
        "!pip install einops           # Tensor dim\n",
        "!pip install torchsummary     # Model summary\n",
        "!pip install thop             # FLOPS check"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCR3wTpB_DF_"
      },
      "source": [
        "\"\"\" Torch library \"\"\"\n",
        "import torch \n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim \n",
        "import torch.backends.cudnn as cudnn \n",
        "import torchvision \n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "\"\"\" Tensor(or data)-related library \"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from einops import rearrange\n",
        "\n",
        "\"\"\" Others \"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "import pdb\n",
        "import albumentations\n",
        "import time\n",
        "import os\n",
        "import csv\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from thop import profile"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sub-design"
      ],
      "metadata": {
        "id": "5jH6rWlK8H4q"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY2VAxS0_luS"
      },
      "source": [
        "\"\"\" Residual connection \"\"\"\n",
        "class Residual(nn.Module):\n",
        "  def __init__(self, fn): \n",
        "    super().__init__()\n",
        "    self.fn = fn \n",
        "\n",
        "  # y = x+f(x) : f(x) is residual info\n",
        "  def forward(self, x):\n",
        "    return self.fn(x) + x"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YZ4yMDiCyX1"
      },
      "source": [
        "\"\"\" Layer normalization \"\"\"\n",
        "class PreNorm(nn.Module): \n",
        "  def __init__(self, dim, fn):\n",
        "    super().__init__()\n",
        "    self.norm = nn.LayerNorm(dim)\n",
        "    self.fn = fn \n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.fn(self.norm(x))  # Layer norm"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdCvfKuaDplu"
      },
      "source": [
        "\"\"\" MLP(Feedforward) layer \"\"\"\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, dim, hidden_dim, dropout=0.):\n",
        "    super().__init__() \n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(dim, hidden_dim), \n",
        "        nn.GELU(), \n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(hidden_dim, dim), \n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "    \n",
        "  # Linear -> GELU -> Dropout -> Linear -> Dropout\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Head Self-Attention layer\n",
        "<img src = \"https://drive.google.com/uc?id=1PYw-0jfRuZR9c5YkZaDNFNGjvzpLA5ww\" height = 300 width = 400>"
      ],
      "metadata": {
        "id": "nnsM1PEMV2I2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Multi-Head Self-Attention \"\"\"\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, dim, heads=8, dropout=0.):\n",
        "    super().__init__()\n",
        "    self.heads = heads \n",
        "    self.scales = heads ** (-0.5)\n",
        "\n",
        "    self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "    self.to_out = nn.Sequential(\n",
        "        nn.Linear(dim, dim), \n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, n, _, h = *x.shape, self.heads\n",
        "\n",
        "    # input vector를 이용해서 query, key, value vector 생성\n",
        "    # (batch, n, dim)이 3개인 tuple 형태\n",
        "    qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "\n",
        "    # 각각의 vector를 head 만큼 split\n",
        "    # (batch, n, dim) -> (batch, head, n, dim/head)\n",
        "    q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=h), qkv)\n",
        "\n",
        "    # compute similarity (scaled dot product) (batch와 head를 유지하면서 행렬곱 진행)\n",
        "    # (batch, head, i, d), (batch, head, j, d) -> (batch, head, i, j) // (i는 query의 개수, j는 key의 개수)\n",
        "    dots = torch.einsum(\"bhid, bhjd->bhij\", q, k) * self.scales\n",
        "\n",
        "    # compute attention weights\n",
        "    # (batch, head, i, j)\n",
        "    attn = dots.softmax(dim=-1)\n",
        "\n",
        "    # compute output vector (batch와 head 유지하면서 행렬곱 진행)\n",
        "    # (batch, head, i, j), (batch, head, i, d) -> (batch, head, i, d)\n",
        "    out = torch.einsum(\"bhij,bhjd->bhid\", attn, v)\n",
        "\n",
        "    # split했던 head들을 concat\n",
        "    # (batch, head, i, d) -> (batch, i, head * d)\n",
        "    out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "\n",
        "    out = self.to_out(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "RhjTmpMOV_Ki"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter & Dataset"
      ],
      "metadata": {
        "id": "Sns7ZkPnUKey"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJCBs6dibF8E"
      },
      "source": [
        "\"\"\"\n",
        "Initial Hyperparameter\n",
        "o learning rate : 0.0001 (10^-4)\n",
        "o batch size : 128\n",
        "o # of epochs : 50\n",
        "\"\"\"\n",
        "\n",
        "lr = 1e-4 \n",
        "bs = 128\n",
        "n_epochs = 50"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Default = 'cuda'\n",
        "best_acc = 0\n",
        "start_epoch = 0"
      ],
      "metadata": {
        "id": "tyiHUTGhYtKC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nxm-zpe9b74o"
      },
      "source": [
        "\"\"\" Make transformed data(set) module \"\"\"\n",
        "transform_train = transforms.Compose([\n",
        "                                     # 이미지를 첫번째 인자의 사이즈로 자름, padding 크기만큼 의미없는 pixel(값 0) 채움\n",
        "                                     transforms.RandomCrop(32, padding=4),\n",
        "                                      \n",
        "                                     # 이미지를 수평으로 뒤집는다\n",
        "                                     transforms.RandomHorizontalFlip(),\n",
        "                                      \n",
        "                                     # 데이터를 tensor로 변환\n",
        "                                     transforms.ToTensor(),\n",
        "                                      \n",
        "                                     # (mean, std) 괄호안의 수의 개수는 채널의 수\n",
        "                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])  # Train\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])  # Test(Validation)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPJ3V1hGcMAT"
      },
      "source": [
        "\"\"\" CIFAR-10 dataset dataloader for test run \"\"\"\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')  # CIFAR-10 classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vanilla ViT\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dsB60BdDd5zm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ViT encoder\n",
        "<img src = \"https://drive.google.com/uc?id=1IkQVsxWBwX28DXHHDFPHMcQd2lOe17b2\" height = 200 width = 100>"
      ],
      "metadata": {
        "id": "BdBAyJ8SJNq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Vanilla ViT encoder \"\"\"\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, dim, depth, heads, mlp_dim, dropout):\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList([]) # Initialize NULL layers\n",
        "\n",
        "    # depth만큼 transformer block 쌓음\n",
        "    for _ in range(depth):\n",
        "      self.layers.append(nn.ModuleList([\n",
        "                                        Residual(PreNorm(dim, Attention(dim, heads=heads, dropout=dropout))), \n",
        "                                        Residual(PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout)))\n",
        "      ])) # 1 encoder * depth\n",
        "\n",
        "  def forward(self, x):\n",
        "    for attn, ff in self.layers:\n",
        "      x = attn(x) # LayerNorm->Attention->Residual connection\n",
        "      x = ff(x)   # LayerNorm->MLP Layer->Residual connection\n",
        "    return x"
      ],
      "metadata": {
        "id": "yaw76YmvvAxB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ViT(Vision Transformer)\n",
        "<img src = \"https://drive.google.com/uc?id=18GXXnl2X3d12q-YUcGa05VrnGAKdA3cb\" height = 200 width = 300>"
      ],
      "metadata": {
        "id": "RcALOITcJoy9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm39nB3qM2qB"
      },
      "source": [
        "MIN_NUM_PATCHES = 16\n",
        "\n",
        "\"\"\" Vanilla ViT \"\"\"\n",
        "class ViT(nn.Module): \n",
        "  def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3, dropout=0., emb_dropout=0.):\n",
        "    super().__init__()\n",
        "\n",
        "    \"\"\" Check parameter \"\"\"\n",
        "    assert image_size % patch_size == 0, \"image size must be divisible for the patch size\"\n",
        "    num_patches = (image_size // patch_size) ** 2\n",
        "    patch_dim = channels * patch_size**2 \n",
        "    assert num_patches >= MIN_NUM_PATCHES,  f'your number of patches ({num_patches}) is way too small for attention to be effective. try decreasing your patch size'\n",
        "\n",
        "    self.patch_size = patch_size \n",
        "\n",
        "    self.pos_embedding = nn.Parameter(torch.randn(1, num_patches+1, dim))\n",
        "    self.patch_to_embedding = nn.Linear(patch_dim, dim, bias=False)\n",
        "    self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "    self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "    self.transformer = Transformer(dim, depth, heads, mlp_dim, dropout) # Multiple encoders\n",
        "    self.to_cls_token = nn.Identity() # Identity (x -> x)\n",
        "\n",
        "    self.mlp_head = nn.Sequential(\n",
        "        nn.LayerNorm(dim), \n",
        "        nn.Linear(dim, mlp_dim), \n",
        "        nn.GELU(), \n",
        "        nn.Dropout(dropout), \n",
        "        nn.Linear(mlp_dim, num_classes)\n",
        "    ) # MLP head\n",
        "\n",
        "  def forward(self, img):\n",
        "    # img shape = (batch size, channel, height, width)\n",
        "\n",
        "    p = self.patch_size\n",
        "    \n",
        "    # 이미지를 패치로 자름, 각 패치를 flatten(1-Dimension) 하게 만듬\n",
        "    # (batch size, channel, height, width) -> (batch size, number of patch, patch_dim)\n",
        "    x = rearrange(img, \"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1=p, p2=p)\n",
        "    \n",
        "    # patch embedding\n",
        "    # (batch size, number of patch, patch_dim) -> (batch size, number of patch, dim)\n",
        "    x = self.patch_to_embedding(x)\n",
        "    b, n, _ = x.shape\n",
        "\n",
        "    # class token embedding vector를 batch size만큼 확장\n",
        "    # (batch size, 1, dim)\n",
        "    cls_tokens = self.cls_token.expand(b, -1, -1)\n",
        "\n",
        "    # x 맨 앞에 cls_tokens 붙힘\n",
        "    # (batch size, number of patch, dim) -> (batch size, number of patch + 1, dim)\n",
        "    x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "    # x에 pos_embedding 더해줌\n",
        "    # shape 같음\n",
        "    x += self.pos_embedding[:, :(n+1)]\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    # trnasformer encoder layer\n",
        "    # (batch size, number of patch + 1, dim) -> (batch size, number of patch + 1, dim)\n",
        "    x = self.transformer(x)\n",
        "\n",
        "    # x 맨앞의 head(class token embedding vector) 추출\n",
        "    # (batch size, number of patch + 1, dim) -> (batch size, dim)\n",
        "    x = self.to_cls_token(x[:, 0])\n",
        "\n",
        "    # mlp layer를 통해 output 구함\n",
        "    # (batch size, dim) -> (batch size, class size(10))\n",
        "    return self.mlp_head(x)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ViT setting"
      ],
      "metadata": {
        "id": "ofJ7uoL-JsAf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0MrYvXWcVHW"
      },
      "source": [
        "\"\"\" Sample ViT using CIFAR-10 \"\"\"\n",
        "net = ViT(\n",
        "    image_size = 32,\n",
        "    patch_size = 4,\n",
        "    num_classes = 10,\n",
        "    dim = 512,\n",
        "    depth = 6,\n",
        "    heads = 8,\n",
        "    mlp_dim = 512,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ")\n",
        "net = net.to(device)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82fmNStSc1m9"
      },
      "source": [
        "\"\"\" ViT Optimizer & Scheduler \"\"\"\n",
        "criterion = nn.CrossEntropyLoss() # Cross entropy loss\n",
        "optimizer = optim.Adam(net.parameters(), lr=lr) # Adam\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, verbose=True, min_lr=1e-3*1e-5, factor=0.1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(net, (3,32,32))"
      ],
      "metadata": {
        "id": "RBjsCPfBVVmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train & Test"
      ],
      "metadata": {
        "id": "A8FXaeBNMJY_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-7hpwswFWlO"
      },
      "source": [
        "def train(model):\n",
        "  model.train()\n",
        "  train_loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    loss = criterion(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "    total += targets.size(0)\n",
        "    correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return train_loss/(batch_idx+1)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2qWNfaNLakI"
      },
      "source": [
        "def test(model):\n",
        "  global best_acc\n",
        "  model.eval() \n",
        "  test_loss = 0 \n",
        "  correct = 0 \n",
        "  total = 0 \n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(testloader): \n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      loss = criterion(outputs, targets)\n",
        "      test_loss += loss.item() \n",
        "      \n",
        "      _, predicted = outputs.max(1)\n",
        "      total += targets.size(0) \n",
        "      correct += predicted.eq(targets).sum().item()\n",
        "      \n",
        "  scheduler.step(test_loss)\n",
        "  acc = 100. * correct/total\n",
        "\n",
        "  if acc > best_acc: \n",
        "    print(\"Best accuracy: {}\\n\".format(acc))\n",
        "  return test_loss, acc"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bes-cdO0jCrO"
      },
      "source": [
        "list_loss_vit = []\n",
        "list_acc_vit = []\n",
        "for epoch in range(n_epochs):\n",
        "  print('Epoch: %d' % epoch)\n",
        "  train(net)\n",
        "  val_loss, val_acc = test(net)\n",
        "\n",
        "  list_loss_vit.append(val_loss)\n",
        "  list_acc_vit.append(val_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_check(model, x):\n",
        "  model.eval() \n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(x)\n",
        "    _, predicted = outputs.max(1)\n",
        "\n",
        "  return predicted"
      ],
      "metadata": {
        "id": "fVYdg6lP6vUe"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######### 0 ~ 9999 ###############\n",
        "num = random.randint(0,9999)\n",
        "\n",
        "compare_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=False)\n",
        "img, _ = compare_set[num]\n",
        "\n",
        "x, label = testset[num]\n",
        "x = x.unsqueeze(dim=0).to(device)\n",
        "\n",
        "predicted = random_check(net, x)\n",
        "\n",
        "print('label:', classes[label])\n",
        "print('predicted:', classes[predicted])\n",
        "plt.imshow(img)\n",
        "print()"
      ],
      "metadata": {
        "id": "XfCHNLaYSVwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT+CSP"
      ],
      "metadata": {
        "id": "KjNCZWhQvD7e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ViT+CSP encoder"
      ],
      "metadata": {
        "id": "YC-RagyHS1Zm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MTocQ7CK6Zx"
      },
      "source": [
        "\"\"\" ViT+CSP encoder \"\"\"\n",
        "class CSP_Transformer(nn.Module):\n",
        "  def __init__(self, dim, depth, heads, mlp_dim, dropout):\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList([]) # Initialize NULL layers\n",
        "    \n",
        "    self.CSPS = Residual(PreNorm(dim, Attention(dim, heads=heads, dropout=dropout)))  # TR-CSP block\n",
        "    self.FFN = Residual(PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout)))\n",
        "\n",
        "    \"\"\" First 3 : TR-CSP \n",
        "        Last  3 : Vanilla attention \"\"\"\n",
        "    for _ in range(depth - 3):\n",
        "      self.layers.append(nn.ModuleList([\n",
        "                                        Residual(PreNorm(dim, Attention(dim, heads=heads, dropout=dropout))), \n",
        "                                        Residual(PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout)))\n",
        "      ]))\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\" 1st TR-CSP \"\"\"\n",
        "    #origin = torch.tensor(x, dtype=torch.int32, device=device)   # Miti-DETR concept (not use)\n",
        "    x1, x2 = x.chunk(2, dim = 1)\n",
        "    x1 = self.CSPS(x1)  # 1st block : x1\n",
        "    x = torch.cat([x1, x2], dim=1)\n",
        "    x = self.FFN(x)\n",
        "    #x += origin   # Miti-DETR concept (not use)\n",
        "\n",
        "    \"\"\" 2nd TR-CSP \"\"\"\n",
        "    #origin = torch.tensor(x, dtype=torch.int32, device=device)\n",
        "    x1, x2 = x.chunk(2, dim = 1)\n",
        "    x2 = self.CSPS(x2)  # 2nd block : x2\n",
        "    x = torch.cat([x1, x2], dim=1)\n",
        "    x = self.FFN(x)\n",
        "    #x += origin\n",
        "\n",
        "    \"\"\" 3rd TR-CSP \"\"\"\n",
        "    #origin = torch.tensor(x, dtype=torch.int32, device=device)\n",
        "    x1, x2 = x.chunk(2, dim = 1)\n",
        "    x1 = self.CSPS(x1)  # 3rd block : x1\n",
        "    x = torch.cat([x1, x2], dim=1)\n",
        "    x = self.FFN(x)\n",
        "    #x += origin\n",
        "\n",
        "    for attn, ff in self.layers:\n",
        "      x = attn(x) # LayerNorm->Attention->Residual connection\n",
        "      x = ff(x)   # LayerNorm->MLP Layer->Residual connection\n",
        "    \n",
        "    return x"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ViT+CSP"
      ],
      "metadata": {
        "id": "7nIH-Wj_S_3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MIN_NUM_PATCHES = 16\n",
        "\n",
        "\"\"\" CSP_ViT \"\"\"\n",
        "class CSP_ViT(nn.Module): \n",
        "  def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3, dropout=0., emb_dropout=0.):\n",
        "    super().__init__()\n",
        "    \n",
        "    \"\"\" Check parameter \"\"\"\n",
        "    assert image_size % patch_size == 0, \"image size must be divisible for the patch size\"\n",
        "    num_patches = (image_size // patch_size) ** 2\n",
        "    patch_dim = channels * patch_size**2 \n",
        "    assert num_patches >= MIN_NUM_PATCHES,  f'your number of patches ({num_patches}) is way too small for attention to be effective. try decreasing your patch size'\n",
        "\n",
        "    self.patch_size = patch_size \n",
        "\n",
        "    self.pos_embedding = nn.Parameter(torch.randn(1, num_patches+1, dim))\n",
        "    self.patch_to_embedding = nn.Linear(patch_dim, dim, bias=False)\n",
        "    self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "    self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "    self.transformer = CSP_Transformer(dim, depth, heads, mlp_dim, dropout) # Multiple encoders\n",
        "    self.to_cls_token = nn.Identity() # Identity (x -> x)\n",
        "\n",
        "    self.mlp_head = nn.Sequential(\n",
        "        nn.LayerNorm(dim), \n",
        "        nn.Linear(dim, mlp_dim), \n",
        "        nn.GELU(), \n",
        "        nn.Dropout(dropout), \n",
        "        nn.Linear(mlp_dim, num_classes)\n",
        "    ) # MLP head\n",
        "\n",
        "  def forward(self, img):\n",
        "    # img shape = (batch size, channel, height, width)\n",
        "\n",
        "    p = self.patch_size\n",
        "    \n",
        "    # 이미지를 패치로 자름, 각 패치를 flatten(1-Dimension) 하게 만듬\n",
        "    # (batch size, channel, height, width) -> (batch size, number of patch, patch_dim)\n",
        "    x = rearrange(img, \"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1=p, p2=p)\n",
        "    \n",
        "    # patch embedding\n",
        "    # (batch size, number of patch, patch_dim) -> (batch size, number of patch, dim)\n",
        "    x = self.patch_to_embedding(x)\n",
        "    b, n, _ = x.shape\n",
        "\n",
        "    # class token embedding vector를 batch size만큼 확장\n",
        "    # (batch size, 1, dim)\n",
        "    cls_tokens = self.cls_token.expand(b, -1, -1)\n",
        "\n",
        "    # x 맨 앞에 cls_tokens 붙힘\n",
        "    # (batch size, number of patch, dim) -> (batch size, number of patch + 1, dim)\n",
        "    x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "    # x에 pos_embedding 더해줌\n",
        "    # shape 같음\n",
        "    x += self.pos_embedding[:, :(n+1)]\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    # trnasformer encoder layer\n",
        "    # (batch size, number of patch + 1, dim) -> (batch size, number of patch + 1, dim)\n",
        "    x = self.transformer(x)\n",
        "\n",
        "    # x 맨앞의 head(class token embedding vector) 추출\n",
        "    # (batch size, number of patch + 1, dim) -> (batch size, dim)\n",
        "    x = self.to_cls_token(x[:, 0])\n",
        "\n",
        "    # mlp layer를 통해 output 구함\n",
        "    # (batch size, dim) -> (batch size, class size(10))\n",
        "    return self.mlp_head(x)"
      ],
      "metadata": {
        "id": "dVBT1W5VvLon"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ViT+CSP setting"
      ],
      "metadata": {
        "id": "k1huJa3zTdup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" ViT + CSP using CIFAR-10 \"\"\"\n",
        "csp_vit_net = CSP_ViT(\n",
        "    image_size = 32,\n",
        "    patch_size = 4,\n",
        "    num_classes = 10,\n",
        "    dim = 512,\n",
        "    depth = 6,\n",
        "    heads = 8,\n",
        "    mlp_dim = 512,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ")\n",
        "csp_vit_net = csp_vit_net.to(device)"
      ],
      "metadata": {
        "id": "F5YDPcNf4PY4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" ViT + CSP Optimizer & Scheduler \"\"\"\n",
        "criterion = nn.CrossEntropyLoss() # Cross entropy loss\n",
        "optimizer = optim.Adam(csp_vit_net.parameters(), lr=lr) # Adam\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, verbose=True, min_lr=1e-3*1e-5, factor=0.1)"
      ],
      "metadata": {
        "id": "gYIyH7Hi3yKh"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(csp_vit_net, (3,32,32))"
      ],
      "metadata": {
        "id": "51aASvbWIUiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train & Test"
      ],
      "metadata": {
        "id": "w5h-IcjtXpIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" ViT + CSP Optimizer & Scheduler \"\"\"\n",
        "list_loss_csps_vit = []\n",
        "list_acc_csps_vit = []\n",
        "for epoch in range(n_epochs):\n",
        "  print('\\nEpoch: %d' % epoch)\n",
        "  train(csp_vit_net)  # train func in ViT\n",
        "  val_loss, val_acc = test(csp_vit_net)  # test func in ViT\n",
        "\n",
        "  list_loss_csps_vit.append(val_loss)\n",
        "  list_acc_csps_vit.append(val_acc)"
      ],
      "metadata": {
        "id": "tU0I20b338BS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######### 0 ~ 9999 ###############\n",
        "num = random.randint(0,9999)\n",
        "\n",
        "compare_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=False)\n",
        "img, _ = compare_set[num]\n",
        "\n",
        "x, label = testset[num]\n",
        "x = x.unsqueeze(dim=0).to(device)\n",
        "\n",
        "predicted = random_check(csp_vit_net, x)\n",
        "\n",
        "print('label:', classes[label])\n",
        "print('predicted:', classes[predicted])\n",
        "plt.imshow(img)\n",
        "print()"
      ],
      "metadata": {
        "id": "PAbiWYx3ZyCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT+CSP ver2"
      ],
      "metadata": {
        "id": "d4vS-vYfSEIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MHA ver2"
      ],
      "metadata": {
        "id": "7BuhWEX2Y7SR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JZwL8aeTz7w"
      },
      "source": [
        "\"\"\" Multi-Head Self-Attention for ViT + CSP ver2 \"\"\"\n",
        "class Attention_ver2(nn.Module): \n",
        "  def __init__(self, dim, dim2, heads=4, dropout=0.):\n",
        "    super().__init__()\n",
        "    self.heads = heads \n",
        "    self.scales = heads ** (-0.5)\n",
        "\n",
        "    self.to_qkv = nn.Linear(dim2, dim2 * 3, bias=False)\n",
        "    self.to_out = nn.Sequential(\n",
        "        nn.Linear(dim2, dim2), \n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, n, _, h = *x.shape, self.heads\n",
        "\n",
        "    # input vector를 이용해서 query, key, value vector 생성\n",
        "    # (batch, n, dim)이 3개인 tuple 형태\n",
        "    qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "\n",
        "    # 각각의 vector를 head 만큼 split\n",
        "    # (batch, n, dim) -> (batch, head, n, dim/head)\n",
        "    q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=h), qkv)\n",
        "\n",
        "    # compute similarity (scaled dot product) (batch와 head를 유지하면서 행렬곱 진행)\n",
        "    # (batch, head, i, d), (batch, head, j, d) -> (batch, head, i, j) // (i는 query의 개수, j는 key의 개수)\n",
        "    dots = torch.einsum(\"bhid, bhjd->bhij\", q, k) * self.scales\n",
        "\n",
        "    # compute attention weights\n",
        "    # (batch, head, i, j)\n",
        "    attn = dots.softmax(dim=-1)\n",
        "\n",
        "    # compute output vector (batch와 head 유지하면서 행렬곱 진행)\n",
        "    # (batch, head, i, j), (batch, head, i, d) -> (batch, head, i, d)\n",
        "    out = torch.einsum(\"bhij,bhjd->bhid\", attn, v)\n",
        "\n",
        "    # split했던 head들을 concat\n",
        "    # (batch, head, i, d) -> (batch, i, head * d)\n",
        "    out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "\n",
        "    out = self.to_out(out)\n",
        "    return out"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ViT+CSP ver2 encoder"
      ],
      "metadata": {
        "id": "8BgcjaFYZA8C"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "504Xm9LaSHYw"
      },
      "source": [
        "\"\"\" ViT + CSP ver2 encoder \"\"\"\n",
        "class CSP_Transformer_ver2(nn.Module):\n",
        "  def __init__(self, dim, dim2, depth, heads, mlp_dim, dropout):\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList([])\n",
        "    \n",
        "    self.CSPS = Residual(PreNorm(dim2, Attention_ver2(dim, dim2, dropout=dropout))) # TR-CSP ver2\n",
        "    self.FFN = Residual(PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout)))\n",
        "\n",
        "    \"\"\" First 3 : TR-CSP ver2\n",
        "        Last  3 : Vanilla attention \"\"\"\n",
        "    for _ in range(depth - 3):\n",
        "      self.layers.append(nn.ModuleList([\n",
        "                                        Residual(PreNorm(dim, Attention(dim, heads=heads, dropout=dropout))), \n",
        "                                        Residual(PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout)))\n",
        "      ]))\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\" 1st TR-CSP ver2 \"\"\"\n",
        "    x1, x2 = x.chunk(2, dim = 2)\n",
        "    x1 = self.CSPS(x1)\n",
        "    x = torch.cat([x1, x2], dim=2)\n",
        "    x = self.FFN(x)\n",
        "\n",
        "    \"\"\" 2nd TR-CSP ver2 \"\"\"\n",
        "    x1, x2 = x.chunk(2, dim = 2)\n",
        "    x2 = self.CSPS(x2)\n",
        "    x = torch.cat([x1, x2], dim=2)\n",
        "    x = self.FFN(x)\n",
        "\n",
        "    \"\"\" 3rd TR-CSP ver2 \"\"\"\n",
        "    x1, x2 = x.chunk(2, dim = 2)\n",
        "    x1 = self.CSPS(x1)\n",
        "    x = torch.cat([x1, x2], dim=2)\n",
        "    x = self.FFN(x)\n",
        "\n",
        "    for attn, ff in self.layers:\n",
        "      x = attn(x) # LayerNorm->Attention->Residual connection\n",
        "      x = ff(x)   # LayerNorm->MLP Layer->Residual connection\n",
        "    return x"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ViT+CSP ver2"
      ],
      "metadata": {
        "id": "qRA80bFqZfS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MIN_NUM_PATCHES = 16\n",
        "\n",
        "\"\"\" CSP_ViT ver2 \"\"\"\n",
        "class CSP_ViT_ver2(nn.Module): \n",
        "  def __init__(self, *, image_size, patch_size, num_classes, dim, dim2, depth, heads, mlp_dim, channels=3, dropout=0., emb_dropout=0.):\n",
        "    super().__init__()\n",
        "    \n",
        "    \"\"\" Check parameter \"\"\"\n",
        "    assert image_size % patch_size == 0, \"image size must be divisible for the patch size\"\n",
        "    num_patches = (image_size // patch_size) ** 2\n",
        "    patch_dim = channels * patch_size**2 \n",
        "    assert num_patches >= MIN_NUM_PATCHES,  f'your number of patches ({num_patches}) is way too small for attention to be effective. try decreasing your patch size'\n",
        "\n",
        "    self.patch_size = patch_size \n",
        "\n",
        "    self.pos_embedding = nn.Parameter(torch.randn(1, num_patches+1, dim))\n",
        "    self.patch_to_embedding = nn.Linear(patch_dim, dim, bias=False)\n",
        "    self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "    self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "    self.transformer = CSP_Transformer_ver2(dim, dim2, depth, heads, mlp_dim, dropout) # Multiple encoders\n",
        "    self.to_cls_token = nn.Identity() # Identity (x -> x)\n",
        "\n",
        "    self.mlp_head = nn.Sequential(\n",
        "        nn.LayerNorm(dim), \n",
        "        nn.Linear(dim, mlp_dim), \n",
        "        nn.GELU(), \n",
        "        nn.Dropout(dropout), \n",
        "        nn.Linear(mlp_dim, num_classes)\n",
        "    ) # MLP head\n",
        "\n",
        "  def forward(self, img):\n",
        "    # img shape = (batch size, channel, height, width)\n",
        "\n",
        "    p = self.patch_size\n",
        "    \n",
        "    # 이미지를 패치로 자름, 각 패치를 flatten(1-Dimension) 하게 만듬\n",
        "    # (batch size, channel, height, width) -> (batch size, number of patch, patch_dim)\n",
        "    x = rearrange(img, \"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1=p, p2=p)\n",
        "    \n",
        "    # patch embedding\n",
        "    # (batch size, number of patch, patch_dim) -> (batch size, number of patch, dim)\n",
        "    x = self.patch_to_embedding(x)\n",
        "    b, n, _ = x.shape\n",
        "\n",
        "    # class token embedding vector를 batch size만큼 확장\n",
        "    # (batch size, 1, dim)\n",
        "    cls_tokens = self.cls_token.expand(b, -1, -1)\n",
        "\n",
        "    # x 맨 앞에 cls_tokens 붙힘\n",
        "    # (batch size, number of patch, dim) -> (batch size, number of patch + 1, dim)\n",
        "    x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "    # x에 pos_embedding 더해줌\n",
        "    # shape 같음\n",
        "    x += self.pos_embedding[:, :(n+1)]\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    # trnasformer encoder layer\n",
        "    # (batch size, number of patch + 1, dim) -> (batch size, number of patch + 1, dim)\n",
        "    x = self.transformer(x)\n",
        "\n",
        "    # x 맨앞의 head(class token embedding vector) 추출\n",
        "    # (batch size, number of patch + 1, dim) -> (batch size, dim)\n",
        "    x = self.to_cls_token(x[:, 0])\n",
        "\n",
        "    # mlp layer를 통해 output 구함\n",
        "    # (batch size, dim) -> (batch size, class size(10))\n",
        "    return self.mlp_head(x)"
      ],
      "metadata": {
        "id": "bRxJ7U0SSXFW"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ViT+CSP ver2 setting"
      ],
      "metadata": {
        "id": "0BW3VavCaoel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" ViT + CSP ver2 using CIFAR-10 \"\"\"\n",
        "csp_vit_ver2_net = CSP_ViT_ver2(\n",
        "    image_size = 32,\n",
        "    patch_size = 4,\n",
        "    num_classes = 10,\n",
        "    dim = 512,\n",
        "    dim2= 256,\n",
        "    depth = 6,\n",
        "    heads = 8,\n",
        "    mlp_dim = 512,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ")\n",
        "csp_vit_ver2_net = csp_vit_ver2_net.to(device)"
      ],
      "metadata": {
        "id": "Q4Q7ZCKrSfTW"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" ViT + CSP ver2 Optimizer & Scheduler \"\"\"\n",
        "criterion = nn.CrossEntropyLoss() # Cross entropy loss\n",
        "optimizer = optim.Adam(csp_vit_ver2_net.parameters(), lr=lr) # Adam\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, verbose=True, min_lr=1e-3*1e-5, factor=0.1)"
      ],
      "metadata": {
        "id": "kDgBlOB-SfTX"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(csp_vit_ver2_net, (3,32,32))"
      ],
      "metadata": {
        "id": "1dbYSQReczvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train & Test"
      ],
      "metadata": {
        "id": "GEpnNR8NWN3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_loss_csps_vit_ver2 = []\n",
        "list_acc_csps_vit_ver2 = []\n",
        "for epoch in range(n_epochs):\n",
        "  print('\\nEpoch: %d' % epoch)\n",
        "  train(csp_vit_ver2_net)\n",
        "  val_loss, val_acc = test(csp_vit_ver2_net)\n",
        "\n",
        "  list_loss_csps_vit_ver2.append(val_loss)\n",
        "  list_acc_csps_vit_ver2.append(val_acc)"
      ],
      "metadata": {
        "id": "M7D6kuM9SfTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######### 0 ~ 9999 ###############\n",
        "num = random.randint(0,9999)\n",
        "\n",
        "compare_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=False)\n",
        "img, _ = compare_set[num]\n",
        "\n",
        "x, label = testset[num]\n",
        "x = x.unsqueeze(dim=0).to(device)\n",
        "\n",
        "predicted = random_check(csp_vit_ver2_net, x)\n",
        "\n",
        "print('label:', classes[label])\n",
        "print('predicted:', classes[predicted])\n",
        "plt.imshow(img)\n",
        "print()"
      ],
      "metadata": {
        "id": "AbWtpEhbZ8de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spec comparison"
      ],
      "metadata": {
        "id": "L4k8b43_TsOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy"
      ],
      "metadata": {
        "id": "9_c8-dOpc894"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "\"\"\" LOSS \"\"\"\n",
        "plt.plot(list_loss_vit, label = \"novel_loss\", color = 'black')\n",
        "plt.plot(list_loss_csps_vit, label = \"csps_loss\", color = 'blue')\n",
        "plt.plot(list_loss_csps_vit_ver2, label = \"csps2_loss\", color = 'skyblue')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\"\"\" ACCURACY \"\"\"\n",
        "plt.plot(list_acc_vit, label = \"novel_accuracy\", color = 'black')\n",
        "plt.plot(list_acc_csps_vit, label = \"csps_accuracy\", color = 'blue')\n",
        "plt.plot(list_acc_csps_vit_ver2, label = \"csps2_accuracy\", color = 'skyblue')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZWmxFHb0YRhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" In colab : download accuracy \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "np.save(F\"/content/gdrive/MyDrive/VIT_result/np_csps_ver2_list_loss.npy\", list_loss_csps_vit_ver2)\n",
        "np.save(F\"/content/gdrive/MyDrive/VIT_result/np_csps_ver2_list_accu.npy\", list_acc_csps_vit_ver2)\n",
        "np.save(F\"/content/gdrive/MyDrive/VIT_result/np_csps_list_loss.npy\", list_loss_csps_vit)\n",
        "np.save(F\"/content/gdrive/MyDrive/VIT_result/np_csps_list_accu.npy\", list_acc_csps_vit)\n",
        "np.save(F\"/content/gdrive/MyDrive/VIT_result/novel_vit_loss.npy\", list_loss)\n",
        "np.save(F\"/content/gdrive/MyDrive/VIT_result/novel_vit_accu.npy\", list_acc)\n",
        "\n",
        "!ls /content/gdrive/MyDrive/VIT_result\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "734IzKMc05P7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FLOPS & Parameters"
      ],
      "metadata": {
        "id": "nblzSMPXfqYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.randn(1, 3, 32, 32) # CIFAR-10 input size\n",
        "input = input.to(device)\n",
        "\n",
        "nobel_flops, nobel_params = profile(net, inputs = (input, ))\n",
        "csps_flops, csps_params = profile(csp_vit_net, inputs = (input, ))\n",
        "csps2_flops, csps2_params = profile(csp_vit_ver2_net, inputs = (input, ))"
      ],
      "metadata": {
        "id": "G5HqdWcCgBNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('FLOPS: ')\n",
        "print('Valilla ViT : FLOPS =', int(nobel_flops), ', parameters =', int(nobel_params))\n",
        "print('ViT+CSP : FLOPS =', int(csps_flops), ', parameters =', int(csps_params))\n",
        "print('CSP ver2 : FLOPS =', int(csps2_flops), ', parameters =', int(csps2_params))"
      ],
      "metadata": {
        "id": "yLHw6qk6hB7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference time"
      ],
      "metadata": {
        "id": "PtstBG9zXE02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_inference_time_ver2(batch):\n",
        "  starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
        "  repetitions = 5000    # Iteration count\n",
        "\n",
        "  novel_vit_inference_time = np.zeros((repetitions,1))\n",
        "  csps_vit_inference_time = np.zeros((repetitions,1))\n",
        "  csps2_vit_inference_time = np.zeros((repetitions,1))\n",
        "\n",
        "  input_image = torch.randn(batch,3,32,32)\n",
        "  input_image = input_image.to(device)\n",
        "\n",
        "  for _ in range(10):\n",
        "    _ = net(input_image)\n",
        "    _ = csp_vit_net(input_image)\n",
        "    _ = csp_vit_ver2_net(input_image)\n",
        "  \n",
        "  pbar = range(repetitions)\n",
        "  # MEASURE PERFORMANCE\n",
        "  with torch.no_grad():\n",
        "    for rep in tqdm(pbar, desc = 'Iteration', total = repetitions, leave = False):\n",
        "        starter.record()\n",
        "        _ = net(input_image)\n",
        "        ender.record()\n",
        "        # WAIT FOR GPU SYNC\n",
        "        torch.cuda.synchronize()\n",
        "        curr_time = starter.elapsed_time(ender)\n",
        "        novel_vit_inference_time[rep] = curr_time\n",
        "        \n",
        "        starter.record()\n",
        "        _ = csp_vit_net(input_image)\n",
        "        ender.record()\n",
        "        # WAIT FOR GPU SYNC\n",
        "        torch.cuda.synchronize()\n",
        "        curr_time = starter.elapsed_time(ender)\n",
        "        csps_vit_inference_time[rep] = curr_time\n",
        "        \n",
        "        starter.record()\n",
        "        _ = csp_vit_ver2_net(input_image)\n",
        "        ender.record()\n",
        "        # WAIT FOR GPU SYNC\n",
        "        torch.cuda.synchronize()\n",
        "        curr_time = starter.elapsed_time(ender)\n",
        "        csps2_vit_inference_time[rep] = curr_time\n",
        "            \n",
        "  input_image = torch.randn(1,3,32,32)\n",
        "  return novel_vit_inference_time, csps_vit_inference_time, csps2_vit_inference_time"
      ],
      "metadata": {
        "id": "FMReDGIgXHGU"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_inference(batch):\n",
        "  title = 'inference time (batch size = ' + str(batch) + ')'\n",
        "\n",
        "  plt.plot(novel_vit_inference_time, label = \"novel_vit\", color = 'black')\n",
        "  plt.plot(csps_vit_inference_time, label = \"csps_vit\", color = 'blue')\n",
        "  plt.plot(csps2_vit_inference_time, label = \"csps2_vit\", color = 'skyblue')\n",
        "\n",
        "  plt.legend()\n",
        "  plt.title(title)\n",
        "  plt.ylabel('inference_time')\n",
        "  plt.show()\n",
        "\n",
        "  print(\"Inference time\")\n",
        "  print('Valilla ViT :', np.average(novel_vit_inference_time))\n",
        "  print('ViT+CSP :', np.average(csps_vit_inference_time))\n",
        "  print('CSP ver2 :', np.average(csps2_vit_inference_time))"
      ],
      "metadata": {
        "id": "eji0AuYJWq59"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" BATCH SIZE = 100 \"\"\"\n",
        "batch = 100\n",
        "novel_vit_inference_time, csps_vit_inference_time, csps2_vit_inference_time = measure_inference_time_ver2(batch)\n",
        "print_inference(batch)\n",
        "\n",
        "\"\"\" BATCH SIZE = 1 \"\"\"\n",
        "batch = 1\n",
        "novel_vit_inference_time, csps_vit_inference_time, csps2_vit_inference_time = measure_inference_time_ver2(batch)\n",
        "print_inference(batch)"
      ],
      "metadata": {
        "id": "xMkAmARAXHGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model save & load (colab only)"
      ],
      "metadata": {
        "id": "MDUncLKdUDZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"from google.colab import drive\n",
        "\n",
        "def SAVE(name, model):\n",
        "  path = F\"/content/gdrive/MyDrive/{name}\"\n",
        "  torch.save(model.state_dict(), path)\n",
        "\n",
        "def LOAD(name, model):\n",
        "  path = F\"/content/gdrive/MyDrive/{name}\"\n",
        "  model.load_state_dict(torch.load(path))\n",
        "\n",
        "drive.mount('/content/gdrive')\"\"\""
      ],
      "metadata": {
        "id": "UkcAI81QU9WX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"SAVE('Vanilla_ViT.pt', net)\n",
        "SAVE('CSP+ViT.pt', csp_vit_net)\n",
        "SAVE('CSP+ViT_ver2.pt', csp_vit_ver2_net)\n",
        "\n",
        "!ls /content/gdrive/MyDrive\"\"\""
      ],
      "metadata": {
        "id": "4N_LfFHQO0LW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"LOAD('Vanilla_ViT.pt', net)\n",
        "LOAD('CSP+ViT.pt', csp_vit_net)\n",
        "LOAD('CSP+ViT_ver2.pt', csp_vit_ver2_net)\"\"\""
      ],
      "metadata": {
        "id": "K0TZ25kkDCCG"
      },
      "execution_count": 45,
      "outputs": []
    }
  ]
}